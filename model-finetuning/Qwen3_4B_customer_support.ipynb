{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31154,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Qwen3-4B-customer-support",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-21T06:08:21.758091Z",
          "iopub.execute_input": "2025-10-21T06:08:21.758733Z",
          "iopub.status.idle": "2025-10-21T06:08:22.061547Z",
          "shell.execute_reply.started": "2025-10-21T06:08:21.758704Z",
          "shell.execute_reply": "2025-10-21T06:08:22.06098Z"
        },
        "id": "xwFoH2D9vqFu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import threading, time\n",
        "\n",
        "def keep_alive():\n",
        "    while True:\n",
        "        print(\"Still alive...\", flush=True)\n",
        "        time.sleep(600)  # every 10 minutes\n",
        "\n",
        "# Start background thread\n",
        "thread = threading.Thread(target=keep_alive, daemon=True)\n",
        "thread.start()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-21T06:08:22.062524Z",
          "iopub.execute_input": "2025-10-21T06:08:22.06286Z",
          "iopub.status.idle": "2025-10-21T06:08:22.068592Z",
          "shell.execute_reply.started": "2025-10-21T06:08:22.062841Z",
          "shell.execute_reply": "2025-10-21T06:08:22.068017Z"
        },
        "id": "tcTTeudivqF0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 1: Installation and Setup\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"Installing Unsloth and dependencies...\")\n",
        "# subprocess.check_call([\n",
        "#     sys.executable, \"-m\", \"pip\", \"install\", \"unsloth\", \"unsloth_zoo\", \"-q\"\n",
        "# ])\n",
        "\n",
        "# subprocess.check_call([\n",
        "#     sys.executable, \"-m\", \"pip\", \"install\",\n",
        "#     \"xformers\", \"trl\", \"peft\", \"accelerate\", \"bitsandbytes\", \"-q\"\n",
        "# ])\n",
        "\n",
        "\n",
        "\n",
        "print(\"Installation complete!\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-21T06:08:22.069197Z",
          "iopub.execute_input": "2025-10-21T06:08:22.069378Z",
          "iopub.status.idle": "2025-10-21T06:08:22.082364Z",
          "shell.execute_reply.started": "2025-10-21T06:08:22.069363Z",
          "shell.execute_reply": "2025-10-21T06:08:22.08179Z"
        },
        "id": "DQ91CGc1vqF2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-21T06:08:22.083764Z",
          "iopub.execute_input": "2025-10-21T06:08:22.083951Z",
          "iopub.status.idle": "2025-10-21T06:08:34.842072Z",
          "shell.execute_reply.started": "2025-10-21T06:08:22.083935Z",
          "shell.execute_reply": "2025-10-21T06:08:34.841106Z"
        },
        "id": "NVRvKVDRvqF4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 2: Import Libraries\n",
        "# ============================================================================\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "import torch\n",
        "from typing import Dict, List\n",
        "import json\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-21T06:08:40.176332Z",
          "iopub.execute_input": "2025-10-21T06:08:40.17717Z",
          "iopub.status.idle": "2025-10-21T06:08:56.98931Z",
          "shell.execute_reply.started": "2025-10-21T06:08:40.177143Z",
          "shell.execute_reply": "2025-10-21T06:08:56.988495Z"
        },
        "id": "Q7hi9DD1vqF7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 3: Configuration\n",
        "# ============================================================================\n",
        "\n",
        "class Config:\n",
        "    # Model Configuration\n",
        "    MODEL_NAME = \"unsloth/Qwen3-4B-Instruct-2507\"\n",
        "    MAX_SEQ_LENGTH = 2048\n",
        "    LOAD_IN_4BIT = True\n",
        "\n",
        "    # LoRA Configuration (memory efficient)\n",
        "    LORA_R = 16\n",
        "    LORA_ALPHA = 16\n",
        "    LORA_DROPOUT = 0.05\n",
        "    TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "\n",
        "    # Training Configuration\n",
        "    BATCH_SIZE = 2\n",
        "    GRADIENT_ACCUMULATION_STEPS = 4\n",
        "    LEARNING_RATE = 2e-4\n",
        "    NUM_EPOCHS = 3\n",
        "    WARMUP_STEPS = 50\n",
        "    MAX_STEPS = -1\n",
        "    SAVE_STEPS = 100\n",
        "    LOGGING_STEPS = 10\n",
        "\n",
        "    # Dataset Configuration\n",
        "    DATASET_NAME = \"bitext/Bitext-customer-support-llm-chatbot-training-dataset\"\n",
        "    DATASET_SPLIT = \"train\"\n",
        "\n",
        "    # Output Configuration\n",
        "    OUTPUT_DIR = \"./qwen3_4B_customer_support_model\"\n",
        "    GGUF_OUTPUT_DIR = \"./qwen3_4B_customer_support_gguf_models\"\n",
        "\n",
        "    # Checkpoint Management (for storage efficiency)\n",
        "    SAVE_TOTAL_LIMIT = 2  # Keep only 2 best checkpoints\n",
        "    RESUME_FROM_CHECKPOINT = True  # Auto-resume from last checkpoint\n",
        "\n",
        "    # HuggingFace Hub Configuration (for backup & resume)\n",
        "    PUSH_TO_HUB = False  # Set to True to enable\n",
        "    HF_REPO_NAME = \"ragib01/Qwen3-4B-customer-support\"  # Change this\n",
        "    PUSH_TO_HUB_MODEL_ID = None  # Will use HF_REPO_NAME if None\n",
        "\n",
        "    # Weights & Biases Configuration (for logging & backup)\n",
        "    USE_WANDB = True  # Set to True to enable\n",
        "    WANDB_PROJECT = \"qwen3-4B-customer-support\"\n",
        "    WANDB_RUN_NAME = None  # Will auto-generate if None\n",
        "\n",
        "config = Config()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-21T06:08:56.990232Z",
          "iopub.execute_input": "2025-10-21T06:08:56.990544Z",
          "iopub.status.idle": "2025-10-21T06:08:56.996971Z",
          "shell.execute_reply.started": "2025-10-21T06:08:56.990519Z",
          "shell.execute_reply": "2025-10-21T06:08:56.996287Z"
        },
        "id": "9HcQIYcqvqF9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 4: Setup HuggingFace Hub & Weights & Biases\n",
        "# ============================================================================\n",
        "\n",
        "# Initialize Weights & Biases\n",
        "if config.USE_WANDB:\n",
        "    try:\n",
        "        import wandb\n",
        "        wandb.init(\n",
        "            project=config.WANDB_PROJECT,\n",
        "            name=config.WANDB_RUN_NAME,\n",
        "            config={\n",
        "                \"model\": config.MODEL_NAME,\n",
        "                \"lora_r\": config.LORA_R,\n",
        "                \"batch_size\": config.BATCH_SIZE,\n",
        "                \"learning_rate\": config.LEARNING_RATE,\n",
        "                \"epochs\": config.NUM_EPOCHS,\n",
        "            }\n",
        "        )\n",
        "        print(\"âœ“ Weights & Biases initialized\")\n",
        "        print(f\"  Project: {config.WANDB_PROJECT}\")\n",
        "        print(f\"  Run: {wandb.run.name}\\n\")\n",
        "    except ImportError:\n",
        "        print(\"âš  wandb not installed. Install with: pip install wandb\")\n",
        "        config.USE_WANDB = False\n",
        "    except Exception as e:\n",
        "        print(f\"âš  Failed to initialize wandb: {e}\")\n",
        "        config.USE_WANDB = False\n",
        "\n",
        "# Setup HuggingFace Hub\n",
        "if config.PUSH_TO_HUB:\n",
        "    try:\n",
        "        from huggingface_hub import HfApi, login, create_repo\n",
        "        print(\"Setting up HuggingFace Hub...\")\n",
        "        print(\"Please login to HuggingFace (if not already logged in)\")\n",
        "        # Will use token from HF_TOKEN env var or prompt for login\n",
        "        try:\n",
        "            api = HfApi()\n",
        "            whoami = api.whoami()\n",
        "            print(f\"âœ“ Logged in as: {whoami['name']}\")\n",
        "        except:\n",
        "            print(\"Please login:\")\n",
        "            login()\n",
        "\n",
        "        # Create repo if it doesn't exist\n",
        "        try:\n",
        "            create_repo(config.HF_REPO_NAME, exist_ok=True, private=True)\n",
        "            print(f\"âœ“ Repository ready: {config.HF_REPO_NAME}\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš  Repository setup: {e}\\n\")\n",
        "    except ImportError:\n",
        "        print(\"âš  huggingface_hub not installed. Install with: pip install huggingface_hub\")\n",
        "        config.PUSH_TO_HUB = False"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-21T06:08:56.997704Z",
          "iopub.execute_input": "2025-10-21T06:08:56.997964Z",
          "iopub.status.idle": "2025-10-21T06:09:10.362026Z",
          "shell.execute_reply.started": "2025-10-21T06:08:56.997946Z",
          "shell.execute_reply": "2025-10-21T06:09:10.361369Z"
        },
        "id": "QiPNky1gvqGF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for existing checkpoints to resume from\n",
        "resume_from_checkpoint = None\n",
        "if config.RESUME_FROM_CHECKPOINT and os.path.exists(config.OUTPUT_DIR):\n",
        "    import glob\n",
        "    checkpoints = glob.glob(os.path.join(config.OUTPUT_DIR, \"checkpoint-*\"))\n",
        "    if checkpoints:\n",
        "        # Sort by modification time, get the latest\n",
        "        latest_checkpoint = max(checkpoints, key=os.path.getmtime)\n",
        "        resume_from_checkpoint = latest_checkpoint\n",
        "        print(\"=\"*70)\n",
        "        print(\"RESUMING FROM CHECKPOINT\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"Found existing checkpoint: {resume_from_checkpoint}\")\n",
        "        print(\"Training will continue from where it left off.\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-21T06:09:10.36285Z",
          "iopub.execute_input": "2025-10-21T06:09:10.363147Z",
          "iopub.status.idle": "2025-10-21T06:09:10.372169Z",
          "shell.execute_reply.started": "2025-10-21T06:09:10.363117Z",
          "shell.execute_reply": "2025-10-21T06:09:10.371455Z"
        },
        "id": "D7BC7y8zvqGI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 5: Load Model with Memory Efficiency\n",
        "# ============================================================================\n",
        "\n",
        "print(\"Loading model with 4-bit quantization for memory efficiency...\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=config.MODEL_NAME,\n",
        "    max_seq_length=config.MAX_SEQ_LENGTH,\n",
        "    dtype=None,\n",
        "    load_in_4bit=config.LOAD_IN_4BIT,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "print(f\"Model loaded: {config.MODEL_NAME}\")\n",
        "print(f\"Max sequence length: {config.MAX_SEQ_LENGTH}\")\n",
        "print(f\"4-bit quantization: {config.LOAD_IN_4BIT}\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-21T06:09:10.37452Z",
          "iopub.execute_input": "2025-10-21T06:09:10.374744Z",
          "iopub.status.idle": "2025-10-21T06:09:18.637634Z",
          "shell.execute_reply.started": "2025-10-21T06:09:10.374727Z",
          "shell.execute_reply": "2025-10-21T06:09:18.636909Z"
        },
        "id": "3uq7ocg0vqGK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 5: Configure LoRA for Memory-Efficient Training\n",
        "# ============================================================================\n",
        "\n",
        "print(\"Configuring LoRA adapters...\")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=config.LORA_R,\n",
        "    target_modules=config.TARGET_MODULES,\n",
        "    lora_alpha=config.LORA_ALPHA,\n",
        "    lora_dropout=config.LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "print(\"LoRA configuration complete!\")\n",
        "print(f\"LoRA rank (r): {config.LORA_R}\")\n",
        "print(f\"LoRA alpha: {config.LORA_ALPHA}\")\n",
        "print(f\"Target modules: {config.TARGET_MODULES}\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-21T06:09:18.63831Z",
          "iopub.execute_input": "2025-10-21T06:09:18.638547Z",
          "iopub.status.idle": "2025-10-21T06:09:25.331201Z",
          "shell.execute_reply.started": "2025-10-21T06:09:18.638528Z",
          "shell.execute_reply": "2025-10-21T06:09:25.330521Z"
        },
        "id": "07ioJ1cavqGL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 6: Load and Prepare Dataset\n",
        "# ============================================================================\n",
        "\n",
        "print(f\"Loading dataset: {config.DATASET_NAME}...\")\n",
        "\n",
        "dataset = load_dataset(config.DATASET_NAME, split=config.DATASET_SPLIT)\n",
        "\n",
        "print(f\"Dataset loaded: {len(dataset)} samples\")\n",
        "print(f\"Dataset columns: {dataset.column_names}\")\n",
        "print(f\"\\nSample data:\")\n",
        "print(dataset[0])\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-21T06:09:25.331909Z",
          "iopub.execute_input": "2025-10-21T06:09:25.33213Z",
          "iopub.status.idle": "2025-10-21T06:09:26.722912Z",
          "shell.execute_reply.started": "2025-10-21T06:09:25.332112Z",
          "shell.execute_reply": "2025-10-21T06:09:26.722165Z"
        },
        "id": "BYodwgY0vqGM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 7: Enhanced Entity Extraction & Tool-Calling Format\n",
        "# ============================================================================\n",
        "import re\n",
        "import random\n",
        "def generate_realistic_value(placeholder):\n",
        "    \"\"\"Generate realistic values for entity placeholders\"\"\"\n",
        "    placeholder_lower = placeholder.lower()\n",
        "\n",
        "    mappings = {\n",
        "        'order number': lambda: f\"#{random.randint(10000000, 99999999)}\",\n",
        "        'invoice number': lambda: f\"INV-{random.randint(1000, 9999)}\",\n",
        "        'tracking number': lambda: f\"TRK{random.randint(100000000, 999999999)}\",\n",
        "        'account number': lambda: f\"ACC{random.randint(100000, 999999)}\",\n",
        "        'refund amount': lambda: f\"${random.randint(10, 500)}.{random.randint(0, 99):02d}\",\n",
        "        'money amount': lambda: f\"${random.randint(10, 500)}.{random.randint(0, 99):02d}\",\n",
        "        'date': lambda: f\"{random.choice(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun'])} {random.randint(1, 28)}, 2024\",\n",
        "        'date range': lambda: f\"{random.choice(['Jan', 'Feb', 'Mar'])} {random.randint(1, 15)} - {random.randint(16, 28)}, 2024\",\n",
        "        'email': lambda: f\"customer{random.randint(100, 999)}@email.com\",\n",
        "        'phone': lambda: f\"+1-{random.randint(200, 999)}-{random.randint(100, 999)}-{random.randint(1000, 9999)}\",\n",
        "        'client first name': lambda: random.choice(['John', 'Sarah', 'Michael', 'Emma', 'David', 'Lisa']),\n",
        "        'client last name': lambda: random.choice(['Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Garcia']),\n",
        "        'delivery city': lambda: random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix']),\n",
        "        'delivery country': lambda: random.choice(['USA', 'Canada', 'UK', 'Australia']),\n",
        "        'account type': lambda: random.choice(['Premium', 'Standard', 'Pro', 'Basic']),\n",
        "        'account category': lambda: random.choice(['Gold', 'Silver', 'Bronze', 'Platinum']),\n",
        "    }\n",
        "\n",
        "    for key, generator in mappings.items():\n",
        "        if key in placeholder_lower:\n",
        "            return generator()\n",
        "\n",
        "    return f\"VALUE{random.randint(1000, 9999)}\"\n",
        "\n",
        "def extract_entities_from_text(text):\n",
        "    \"\"\"Extract all {{entity}} placeholders and replace with realistic values\"\"\"\n",
        "    placeholders = re.findall(r'\\{\\{([^}]+)\\}\\}', text)\n",
        "    entity_map = {}\n",
        "\n",
        "    for placeholder in placeholders:\n",
        "        if placeholder not in entity_map:\n",
        "            entity_map[placeholder] = generate_realistic_value(placeholder)\n",
        "\n",
        "    replaced_text = text\n",
        "    for placeholder, value in entity_map.items():\n",
        "        replaced_text = replaced_text.replace(f\"{{{{{placeholder}}}}}\", value)\n",
        "\n",
        "    return replaced_text, entity_map\n",
        "\n",
        "def get_tool_for_intent(intent, category):\n",
        "    \"\"\"Determine which tool should be called for a given intent\"\"\"\n",
        "    tool_mapping = {\n",
        "        'track_order': {\n",
        "            'name': 'track_order',\n",
        "            'description': 'Track the status of an order',\n",
        "            'parameters': ['order_number']\n",
        "        },\n",
        "        'cancel_order': {\n",
        "            'name': 'cancel_order',\n",
        "            'description': 'Cancel an existing order',\n",
        "            'parameters': ['order_number']\n",
        "        },\n",
        "        'get_invoice': {\n",
        "            'name': 'get_invoice',\n",
        "            'description': 'Retrieve invoice details',\n",
        "            'parameters': ['invoice_number', 'order_number']\n",
        "        },\n",
        "        'check_invoice': {\n",
        "            'name': 'check_invoice',\n",
        "            'description': 'Check invoice information',\n",
        "            'parameters': ['invoice_number']\n",
        "        },\n",
        "        'track_refund': {\n",
        "            'name': 'track_refund',\n",
        "            'description': 'Track refund status',\n",
        "            'parameters': ['order_number', 'refund_id']\n",
        "        },\n",
        "        'get_refund': {\n",
        "            'name': 'get_refund',\n",
        "            'description': 'Process a refund request',\n",
        "            'parameters': ['order_number']\n",
        "        },\n",
        "        'check_payment_methods': {\n",
        "            'name': 'get_payment_methods',\n",
        "            'description': 'Get available payment methods',\n",
        "            'parameters': []\n",
        "        },\n",
        "        'delivery_options': {\n",
        "            'name': 'get_delivery_options',\n",
        "            'description': 'Get available delivery options',\n",
        "            'parameters': ['location']\n",
        "        },\n",
        "        'check_refund_policy': {\n",
        "            'name': 'get_refund_policy',\n",
        "            'description': 'Get refund policy details',\n",
        "            'parameters': []\n",
        "        },\n",
        "    }\n",
        "\n",
        "    return tool_mapping.get(intent, None)\n",
        "\n",
        "def create_tool_calling_format(instruction, response, intent, category, entity_map):\n",
        "    \"\"\"Create training format with tool-calling capability\"\"\"\n",
        "\n",
        "    tool_info = get_tool_for_intent(intent, category)\n",
        "\n",
        "    # Determine if tool should be called\n",
        "    needs_tool = tool_info is not None\n",
        "\n",
        "    if needs_tool:\n",
        "        # Extract parameters from entity_map\n",
        "        parameters = {}\n",
        "        for param in tool_info['parameters']:\n",
        "            # Try to find matching entity\n",
        "            for entity_name, entity_value in entity_map.items():\n",
        "                if param.lower() in entity_name.lower():\n",
        "                    parameters[param] = entity_value\n",
        "\n",
        "        # Create tool call format\n",
        "        tool_call = {\n",
        "            \"name\": tool_info['name'],\n",
        "            \"arguments\": parameters\n",
        "        }\n",
        "\n",
        "        # Format with tool calling\n",
        "        formatted_text = f\"\"\"<|im_start|>system\n",
        "You are a helpful customer support assistant with access to tools. When users ask about orders, invoices, refunds, or account information, use the appropriate tool to retrieve accurate data. Always extract exact values (like order numbers) from the user's message.<|im_end|>\n",
        "<|im_start|>user\n",
        "{instruction}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "I'll help you with that. Let me check the information for you.\n",
        "\n",
        "<tool_call>\n",
        "{json.dumps(tool_call, indent=2)}\n",
        "</tool_call>\n",
        "\n",
        "Based on the information retrieved: {response}<|im_end|>\"\"\"\n",
        "\n",
        "    else:\n",
        "        # No tool needed, standard response\n",
        "        formatted_text = f\"\"\"<|im_start|>system\n",
        "You are a helpful customer support assistant. When responding, use the exact values provided by the user (like order numbers, dates, etc.) in your replies.<|im_end|>\n",
        "<|im_start|>user\n",
        "{instruction}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "{response}<|im_end|>\"\"\"\n",
        "\n",
        "    return formatted_text\n",
        "\n",
        "def format_dataset_entry(example: Dict) -> Dict:\n",
        "    \"\"\"\n",
        "    Enhanced formatting with:\n",
        "    1. Entity extraction (replace {{placeholders}})\n",
        "    2. Tool-calling format\n",
        "    3. Value preservation training\n",
        "    \"\"\"\n",
        "\n",
        "    instruction = example.get(\"instruction\", \"\")\n",
        "    response = example.get(\"response\", \"\")\n",
        "    intent = example.get(\"intent\", \"\")\n",
        "    category = example.get(\"category\", \"\")\n",
        "\n",
        "    # Replace placeholders with realistic values\n",
        "    instruction_replaced, entity_map = extract_entities_from_text(instruction)\n",
        "\n",
        "    # Replace placeholders in response using same entity values\n",
        "    response_replaced = response\n",
        "    for placeholder, value in entity_map.items():\n",
        "        response_replaced = response_replaced.replace(f\"{{{{{placeholder}}}}}\", value)\n",
        "\n",
        "    # Also replace any remaining placeholders\n",
        "    response_replaced, _ = extract_entities_from_text(response_replaced)\n",
        "\n",
        "    # Create format with tool calling awareness\n",
        "    formatted_text = create_tool_calling_format(\n",
        "        instruction_replaced,\n",
        "        response_replaced,\n",
        "        intent,\n",
        "        category,\n",
        "        entity_map\n",
        "    )\n",
        "\n",
        "    return {\"text\": formatted_text}\n",
        "\n",
        "print(\"Applying enhanced formatting with entity extraction and tool-calling...\")\n",
        "print(\"This teaches the model to:\")\n",
        "print(\"  1. Extract actual values from user input (not placeholders)\")\n",
        "print(\"  2. Use tool-calling for data retrieval\")\n",
        "print(\"  3. Preserve exact values in responses\\n\")\n",
        "\n",
        "# Apply formatting to dataset\n",
        "formatted_dataset = dataset.map(\n",
        "    format_dataset_entry,\n",
        "    remove_columns=dataset.column_names,\n",
        "    desc=\"Formatting dataset with entity extraction and tool-calling\"\n",
        ")\n",
        "\n",
        "print(f\"âœ“ Dataset formatted: {len(formatted_dataset)} samples\\n\")\n",
        "\n",
        "# Show examples of different formats\n",
        "print(\"=\"*70)\n",
        "print(\"SAMPLE TRAINING EXAMPLES\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nExample 1 (with tool calling):\")\n",
        "print(formatted_dataset[0][\"text\"][:800])\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "# Split dataset for training and validation\n",
        "dataset_split = formatted_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = dataset_split[\"train\"]\n",
        "eval_dataset = dataset_split[\"test\"]\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(eval_dataset)}\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-21T06:09:26.723844Z",
          "iopub.execute_input": "2025-10-21T06:09:26.724219Z",
          "iopub.status.idle": "2025-10-21T06:09:26.771037Z",
          "shell.execute_reply.started": "2025-10-21T06:09:26.724192Z",
          "shell.execute_reply": "2025-10-21T06:09:26.770473Z"
        },
        "id": "kL-bS-BTvqGN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 8: Configure Training Arguments\n",
        "# ============================================================================\n",
        "\n",
        "print(\"Configuring training parameters...\")\n",
        "\n",
        "# Detect number of GPUs available\n",
        "num_gpus = torch.cuda.device_count()\n",
        "print(f\"Number of GPUs detected: {num_gpus}\")\n",
        "\n",
        "# Adjust batch size for multi-GPU if available\n",
        "# For multi-GPU, you can increase per_device batch size or keep it same\n",
        "# Total effective batch size = per_device_batch_size * num_gpus * gradient_accumulation_steps\n",
        "if num_gpus > 1:\n",
        "    print(f\"Multi-GPU training enabled with {num_gpus} GPUs\")\n",
        "    print(f\"Effective batch size: {config.BATCH_SIZE} * {num_gpus} * {config.GRADIENT_ACCUMULATION_STEPS} = {config.BATCH_SIZE * num_gpus * config.GRADIENT_ACCUMULATION_STEPS}\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=config.OUTPUT_DIR,\n",
        "    per_device_train_batch_size=config.BATCH_SIZE,\n",
        "    per_device_eval_batch_size=config.BATCH_SIZE,\n",
        "    gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n",
        "    warmup_steps=config.WARMUP_STEPS,\n",
        "    max_steps=config.MAX_STEPS,\n",
        "    num_train_epochs=config.NUM_EPOCHS,\n",
        "    learning_rate=config.LEARNING_RATE,\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    logging_steps=config.LOGGING_STEPS,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    seed=3407,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=config.SAVE_STEPS,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=config.SAVE_STEPS,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    # Checkpoint management (save space)\n",
        "    save_total_limit=config.SAVE_TOTAL_LIMIT,  # Keep only N best checkpoints\n",
        "    # Logging configuration\n",
        "    report_to=\"wandb\" if config.USE_WANDB else \"none\",\n",
        "    run_name=\"qwen_customer_support_finetuning\",\n",
        "    # HuggingFace Hub configuration\n",
        "    push_to_hub=config.PUSH_TO_HUB,\n",
        "    hub_model_id=config.HF_REPO_NAME if config.PUSH_TO_HUB else None,\n",
        "    hub_strategy=\"checkpoint\",  # Push every checkpoint\n",
        "    hub_private_repo=True,\n",
        ")\n",
        "\n",
        "print(\"Training configuration:\")\n",
        "print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
        "print(f\"  Gradient accumulation: {config.GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"  Effective batch size: {config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"  Learning rate: {config.LEARNING_RATE}\")\n",
        "print(f\"  Epochs: {config.NUM_EPOCHS}\")\n",
        "print(f\"  Optimizer: adamw_8bit (memory efficient)\")\n",
        "print(f\"  Mixed precision: {'bf16' if torch.cuda.is_bf16_supported() else 'fp16'}\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-21T06:09:26.771752Z",
          "iopub.execute_input": "2025-10-21T06:09:26.771978Z",
          "iopub.status.idle": "2025-10-21T06:09:26.814723Z",
          "shell.execute_reply.started": "2025-10-21T06:09:26.771952Z",
          "shell.execute_reply": "2025-10-21T06:09:26.813963Z"
        },
        "id": "_NLcGJNnvqGQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 9: Initialize Trainer\n",
        "# ============================================================================\n",
        "\n",
        "print(\"Initializing SFT Trainer...\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=config.MAX_SEQ_LENGTH,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized successfully!\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-21T06:09:26.815632Z",
          "iopub.execute_input": "2025-10-21T06:09:26.815871Z",
          "iopub.status.idle": "2025-10-21T06:09:27.308554Z",
          "shell.execute_reply.started": "2025-10-21T06:09:26.815854Z",
          "shell.execute_reply": "2025-10-21T06:09:27.307743Z"
        },
        "id": "rmOn-c8avqGY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 10: Training\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\" * 70)\n",
        "print()\n",
        "\n",
        "# Enable memory efficient training\n",
        "model.config.use_cache = False\n",
        "\n",
        "# Train the model (with resume capability)\n",
        "trainer_stats = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nTraining time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
        "print(f\"Training samples per second: {trainer_stats.metrics['train_samples_per_second']:.2f}\")\n",
        "print(f\"Final training loss: {trainer_stats.metrics['train_loss']:.4f}\")\n",
        "\n",
        "# Show checkpoint management info\n",
        "if config.SAVE_TOTAL_LIMIT:\n",
        "    checkpoints = glob.glob(os.path.join(config.OUTPUT_DIR, \"checkpoint-*\"))\n",
        "    print(f\"\\nCheckpoint Management:\")\n",
        "    print(f\"  Total checkpoints saved: {len(checkpoints)}\")\n",
        "    print(f\"  Limit: {config.SAVE_TOTAL_LIMIT} (older checkpoints auto-deleted)\")\n",
        "    if checkpoints:\n",
        "        print(f\"  Latest: {max(checkpoints, key=os.path.getmtime)}\")\n",
        "\n",
        "# Finish Weights & Biases\n",
        "if config.USE_WANDB:\n",
        "    try:\n",
        "        import wandb\n",
        "        wandb.finish()\n",
        "        print(\"\\nâœ“ Weights & Biases session finished\")\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-21T14:47:04.662734Z",
          "iopub.status.idle": "2025-10-21T14:47:04.663018Z",
          "shell.execute_reply.started": "2025-10-21T14:47:04.662873Z",
          "shell.execute_reply": "2025-10-21T14:47:04.662884Z"
        },
        "id": "snbrRQcQvqGZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 11: Save Model\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"SAVING MODEL\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Save LoRA adapters\n",
        "print(\"\\n1. Saving LoRA adapters...\")\n",
        "model.save_pretrained(f\"{config.OUTPUT_DIR}/lora_adapters\")\n",
        "tokenizer.save_pretrained(f\"{config.OUTPUT_DIR}/lora_adapters\")\n",
        "print(f\"   Saved to: {config.OUTPUT_DIR}/lora_adapters\")\n",
        "\n",
        "# Save merged model (16-bit)\n",
        "print(\"\\n2. Saving merged 16-bit model...\")\n",
        "model.save_pretrained_merged(\n",
        "    f\"{config.OUTPUT_DIR}/merged_16bit\",\n",
        "    tokenizer,\n",
        "    save_method=\"merged_16bit\",\n",
        ")\n",
        "print(f\"   Saved to: {config.OUTPUT_DIR}/merged_16bit\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-21T14:47:05.154682Z",
          "iopub.execute_input": "2025-10-21T14:47:05.154982Z",
          "iopub.status.idle": "2025-10-21T14:48:33.525219Z",
          "shell.execute_reply.started": "2025-10-21T14:47:05.154959Z",
          "shell.execute_reply": "2025-10-21T14:48:33.52457Z"
        },
        "id": "6gO_G6v9vqGb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 12: GGUF Export\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"EXPORTING TO GGUF FORMAT\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "os.makedirs(config.GGUF_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Export to various GGUF quantization formats\n",
        "quantization_methods = [\n",
        "    \"q4_k_m\",  # 4-bit quantization, medium quality (recommended for most use cases)\n",
        "    \"q5_k_m\",  # 5-bit quantization, medium quality (better quality)\n",
        "    \"q8_0\",    # 8-bit quantization (highest quality)\n",
        "]\n",
        "\n",
        "print(\"\\nExporting to multiple GGUF quantization formats...\\n\")\n",
        "\n",
        "for quant_method in quantization_methods:\n",
        "    print(f\"Exporting to {quant_method.upper()}...\")\n",
        "    try:\n",
        "        model.save_pretrained_gguf(\n",
        "            f\"{config.GGUF_OUTPUT_DIR}/qwen_customer_support_{quant_method}\",\n",
        "            tokenizer,\n",
        "            quantization_method=quant_method,\n",
        "        )\n",
        "        print(f\"  âœ“ Successfully exported to {quant_method.upper()}\")\n",
        "        print(f\"    Location: {config.GGUF_OUTPUT_DIR}/qwen_customer_support_{quant_method}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  âœ— Failed to export {quant_method.upper()}: {str(e)}\")\n",
        "    print()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-21T15:26:13.055466Z",
          "iopub.execute_input": "2025-10-21T15:26:13.055803Z",
          "iopub.status.idle": "2025-10-21T15:30:28.265151Z",
          "shell.execute_reply.started": "2025-10-21T15:26:13.055777Z",
          "shell.execute_reply": "2025-10-21T15:30:28.264424Z"
        },
        "id": "RNQWAg7CvqGc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, login\n",
        "import os\n",
        "\n",
        "# ==============================================================\n",
        "# Step 1: Login (if not already done)\n",
        "# ==============================================================\n",
        "#login(token=\"YOUR_HF_TOKEN\")  # or run login() interactively\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "# ==============================================================\n",
        "# Step 2: Define repo info\n",
        "# ==============================================================\n",
        "gguf_folder = \"/kaggle/working/qwen3_4B_customer_support_gguf_models\"\n",
        "repo_id = \"ragib01/Qwen3-4B-customer-support-gguf\"  # <- new repo\n",
        "\n",
        "# ==============================================================\n",
        "# Step 3: Create repo (only once)\n",
        "# ==============================================================\n",
        "api.create_repo(repo_id=repo_id, private=False, exist_ok=True)\n",
        "\n",
        "# ==============================================================\n",
        "# Step 4: Upload GGUF files\n",
        "# ==============================================================\n",
        "api.upload_folder(\n",
        "    folder_path=gguf_folder,\n",
        "    repo_id=repo_id,\n",
        "    commit_message=\"Upload Qwen3-4B fine-tuned GGUF quantized model files\",\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… GGUF model uploaded successfully!\")\n",
        "print(f\"ðŸ”— View it at: https://huggingface.co/{repo_id}\")\n",
        "print(\"\\nUsers can now download and run with llama.cpp, LM Studio, or Ollama.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-21T15:31:43.916707Z",
          "iopub.execute_input": "2025-10-21T15:31:43.917647Z",
          "iopub.status.idle": "2025-10-21T15:32:40.804986Z",
          "shell.execute_reply.started": "2025-10-21T15:31:43.917619Z",
          "shell.execute_reply": "2025-10-21T15:32:40.803982Z"
        },
        "id": "5LA6B8VEvqGd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 13: Inference Test\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"TESTING FINE-TUNED MODEL\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Enable inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "test_prompts = [\n",
        "    \"How do I track my order?\",\n",
        "    \"I want to change my shipping address\",\n",
        "    \"What is your return policy?\",\n",
        "    \"How can I contact customer support?\",\n",
        "]\n",
        "\n",
        "print(\"\\nRunning inference tests...\\n\")\n",
        "\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"Test {i}/{len(test_prompts)}\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    # Format with chat template\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful customer support assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    input_text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        use_cache=True,\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the assistant's response\n",
        "    if \"<|im_start|>assistant\" in response:\n",
        "        response = response.split(\"<|im_start|>assistant\")[-1].strip()\n",
        "\n",
        "    print(f\"Response: {response}\")\n",
        "    print(\"-\" * 70)\n",
        "    print()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-21T14:56:19.122996Z",
          "iopub.execute_input": "2025-10-21T14:56:19.123698Z",
          "iopub.status.idle": "2025-10-21T14:56:56.404544Z",
          "shell.execute_reply.started": "2025-10-21T14:56:19.123672Z",
          "shell.execute_reply": "2025-10-21T14:56:56.403798Z"
        },
        "id": "BHOLXHZHvqGe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, login\n",
        "# Log in (if not already done)\n",
        "print(\"\\n[4/5] Logging in to HuggingFace...\")\n",
        "login(token=\"\")  # Replace with your actual token or remove for manual login\n",
        "print(\"âœ“ Logged in\")\n",
        "\n",
        "# Define model folder and repo\n",
        "output_path = \"/kaggle/working/qwen3_4B_customer_support_model/merged_16bit\"\n",
        "repo_id = \"ragib01/Qwen3-4B-customer-support\"\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "print(\"\\n[5/5] Uploading to HuggingFace Hub...\")\n",
        "\n",
        "# (Optional) clear old files\n",
        "try:\n",
        "    api.delete_folder(repo_id=repo_id, path_in_repo=\"\")\n",
        "    print(\"   Cleared existing files in repo.\")\n",
        "except Exception as e:\n",
        "    print(\"   Skipped clearing:\", e)\n",
        "\n",
        "# Upload merged model\n",
        "api.upload_folder(\n",
        "    folder_path=output_path,\n",
        "    repo_id=repo_id,\n",
        "    commit_message=\"Upload merged 16-bit model (standard transformers format, no Unsloth needed)\"\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… UPLOAD COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nYour model is now available at: https://huggingface.co/{repo_id}\")\n",
        "print(\"\\nLoad it with:\")\n",
        "print('''\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"{repo_id}\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"{repo_id}\", torch_dtype=\"auto\", device_map=\"auto\")\n",
        "\n",
        "model.eval()\n",
        "print(model.generate(**tokenizer(\"Hello!\", return_tensors=\"pt\")))\n",
        "''')\n",
        "print(\"=\"*70)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-21T15:15:46.355806Z",
          "iopub.execute_input": "2025-10-21T15:15:46.356134Z",
          "iopub.status.idle": "2025-10-21T15:16:37.756523Z",
          "shell.execute_reply.started": "2025-10-21T15:15:46.35611Z",
          "shell.execute_reply": "2025-10-21T15:16:37.755625Z"
        },
        "id": "BV1c--KovqGe"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}